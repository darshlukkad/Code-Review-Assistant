{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Model Training with TensorBoard\n",
                "\n",
                "**AI-Powered Code Review Assistant**  \n",
                "**CS 5590 - Final Project**\n",
                "\n",
                "---\n",
                "\n",
                "## Objectives\n",
                "\n",
                "This notebook implements the complete training pipeline:\n",
                "\n",
                "1. **Initialize** CodeBERT model for multi-label classification\n",
                "2. **Configure** training with justified hyperparameters\n",
                "3. **Train** with TensorBoard monitoring\n",
                "4. **Save** best model checkpoints\n",
                "5. **Analyze** training curves\n",
                "\n",
                "---\n",
                "\n",
                "## CRISP-DM Phase: Modeling\n",
                "\n",
                "This notebook corresponds to **Phase 4** of the CRISP-DM methodology.\n",
                "\n",
                "---\n",
                "\n",
                "## GPU Setup (Google Colab)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "    device = torch.device('cuda')\n",
                "else:\n",
                "    print(\"⚠ No GPU available, using CPU (training will be slow)\")\n",
                "    device = torch.device('cpu')\n",
                "\n",
                "print(f\"\\nUsing device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import google.colab\n",
                "    IN_COLAB = True\n",
                "    !git clone https://github.com/darshlukkad/Code-Review-Assistant.git\n",
                "    %cd Code-Review-Assistant\n",
                "except ImportError:\n",
                "    IN_COLAB = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch tensorboard scikit-learn tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('src')\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "from transformers import AdamW, get_linear_schedule_with_warmup\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import json\n",
                "import os\n",
                "\n",
                "# Import our modules\n",
                "from models.model import CodeBERTClassifier\n",
                "from data.preprocessing import CodePreprocessor\n",
                "from training.config import training_config, model_config\n",
                "\n",
                "print(\"✓ All libraries imported\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Preprocessed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data splits from preprocessing notebook\n",
                "from torch.utils.data import DataLoader\n",
                "from data.preprocessing import CodePreprocessor\n",
                "\n",
                "# Load splits\n",
                "train_df = pd.read_csv('train_split.csv')\n",
                "val_df = pd.read_csv('val_split.csv')\n",
                "\n",
                "print(f\"Train: {len(train_df):,} samples\")\n",
                "print(f\"Val:   {len(val_df):,} samples\")\n",
                "\n",
                "# Initialize preprocessor\n",
                "preprocessor = CodePreprocessor()\n",
                "\n",
                "# Create datasets and loaders will be done using preprocessing code\n",
                "print(\"\\n✓ Data loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize Model\n",
                "\n",
                "### Model Architecture: CodeBERT\n",
                "\n",
                "**Why CodeBERT?**\n",
                "- Pre-trained on code from 6 languages (2.1M samples)\n",
                "- Understands code structure better than BERT\n",
                "- State-of-the-art on code understanding tasks\n",
                "\n",
                "**Architecture Details:**\n",
                "- 12 transformer layers\n",
                "- 768 hidden dimensions\n",
                "- 12 attention heads\n",
                "- Multi-label classification head (5 outputs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model\n",
                "model = CodeBERTClassifier(\n",
                "    model_name=\"microsoft/codebert-base\",\n",
                "    num_labels=5,\n",
                "    hidden_dropout_prob=0.1\n",
                ")\n",
                "\n",
                "model = model.to(device)\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "\n",
                "print(\"Model Architecture:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Total parameters:     {total_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")\n",
                "print(f\"Model size:          ~{total_params * 4 / 1e6:.0f} MB\")\n",
                "print(\"\\n✓ Model initialized successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Configure Training Hyperparameters\n",
                "\n",
                "### Complete Hyperparameter Justifications\n",
                "\n",
                "| Hyperparameter | Value | Justification |\n",
                "|----------------|-------|---------------|\n",
                "| **Learning Rate** | 2e-5 | Standard for BERT fine-tuning, provides stable convergence |\n",
                "| **Optimizer** | AdamW | Decoupled weight decay prevents overfitting better than Adam |\n",
                "| **Weight Decay** | 0.01 | Regularization to prevent overfitting, standard for transformers |\n",
                "| **Batch Size** | 32 | Balances GPU memory (16GB) with gradient stability |\n",
                "| **Epochs** | 15 | Sufficient with early stopping (patience=3) |\n",
                "| **Warmup Steps** | 500 | Linear warmup prevents unstable early training |\n",
                "| **Max Grad Norm** | 1.0 | Prevents exploding gradients during training |\n",
                "| **Dropout** | 0.1 | Regularization without losing model capacity |\n",
                "| **Loss Function** | BCEWithLogitsLoss | Multi-label requires independent probabilities per class |\n",
                "| **Activation** | GELU | Smoother gradients than ReLU, standard in transformers |\n",
                "| **Scheduler** | Linear | Gradual learning rate decay after warmup |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "LEARNING_RATE = 2e-5\n",
                "WEIGHT_DECAY = 0.01\n",
                "BATCH_SIZE = 32\n",
                "NUM_EPOCHS = 15\n",
                "WARMUP_STEPS = 500\n",
                "MAX_GRAD_NORM = 1.0\n",
                "EARLY_STOPPING_PATIENCE = 3\n",
                "\n",
                "# Optimizer: AdamW\n",
                "optimizer = AdamW(\n",
                "    model.parameters(),\n",
                "    lr=LEARNING_RATE,\n",
                "    weight_decay=WEIGHT_DECAY,\n",
                "    betas=(0.9, 0.999),\n",
                "    eps=1e-8\n",
                ")\n",
                "\n",
                "# Learning rate scheduler with warmup\n",
                "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
                "scheduler = get_linear_schedule_with_warmup(\n",
                "    optimizer,\n",
                "    num_warmup_steps=WARMUP_STEPS,\n",
                "    num_training_steps=num_training_steps\n",
                ")\n",
                "\n",
                "# Loss function (already in model, but we can override)\n",
                "# BCEWithLogitsLoss combines sigmoid + BCE for numerical stability\n",
                "\n",
                "print(\"Training Configuration:\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Learning rate:        {LEARNING_RATE}\")\n",
                "print(f\"Weight decay:         {WEIGHT_DECAY}\")\n",
                "print(f\"Batch size:           {BATCH_SIZE}\")\n",
                "print(f\"Num epochs:           {NUM_EPOCHS}\")\n",
                "print(f\"Warmup steps:         {WARMUP_STEPS}\")\n",
                "print(f\"Total training steps: {num_training_steps:,}\")\n",
                "print(f\"Early stopping:       {EARLY_STOPPING_PATIENCE} epochs\")\n",
                "print(\"\\n✓ Training configured\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Set Up TensorBoard\n",
                "\n",
                "TensorBoard will log:\n",
                "- Training/validation loss per step\n",
                "- Learning rate schedule\n",
                "- Gradient norms\n",
                "- Model graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize TensorBoard writer\n",
                "writer = SummaryWriter('logs/tensorboard')\n",
                "\n",
                "# Log hyperparameters\n",
                "hparams = {\n",
                "    'learning_rate': LEARNING_RATE,\n",
                "    'weight_decay': WEIGHT_DECAY,\n",
                "    'batch_size': BATCH_SIZE,\n",
                "    'num_epochs': NUM_EPOCHS,\n",
                "    'warmup_steps': WARMUP_STEPS,\n",
                "    'max_grad_norm': MAX_GRAD_NORM\n",
                "}\n",
                "\n",
                "writer.add_hparams(hparams, {})\n",
                "\n",
                "print(\"✓ TensorBoard initialized\")\n",
                "print(\"  Run: tensorboard --logdir=logs/tensorboard\")\n",
                "\n",
                "# In Colab, load TensorBoard extension\n",
                "if IN_COLAB:\n",
                "    %load_ext tensorboard\n",
                "    %tensorboard --logdir logs/tensorboard"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Loop\n",
                "\n",
                "### Training Process:\n",
                "\n",
                "For each epoch:\n",
                "1. **Training phase:**\n",
                "   - Forward pass through model\n",
                "   - Compute loss (BCEWithLogitsLoss)\n",
                "   - Backward pass (compute gradients)\n",
                "   - Clip gradients (prevent explosion)\n",
                "   - Update weights\n",
                "   - Update learning rate\n",
                "\n",
                "2. **Validation phase:**\n",
                "   - Evaluate on validation set\n",
                "   - Check for improvement\n",
                "   - Save best model\n",
                "\n",
                "3. **Early stopping:**\n",
                "   - Stop if no improvement for 3 epochs\n",
                "   - Prevents overfitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, scheduler, device, epoch, writer, global_step):\n",
                "    \"\"\"Train for one epoch.\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
                "    \n",
                "    for batch_idx, batch in enumerate(progress_bar):\n",
                "        # Move to device\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "        \n",
                "        # Forward pass\n",
                "        outputs = model(input_ids, attention_mask, labels)\n",
                "        loss = outputs['loss']\n",
                "        \n",
                "        # Backward pass\n",
                "        loss.backward()\n",
                "        \n",
                "        # Gradient clipping\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
                "        \n",
                "        # Update weights\n",
                "        optimizer.step()\n",
                "        scheduler.step()\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Log to TensorBoard\n",
                "        global_step += 1\n",
                "        if global_step % 100 == 0:\n",
                "            writer.add_scalar('train/loss', loss.item(), global_step)\n",
                "            writer.add_scalar('train/lr', scheduler.get_last_lr()[0], global_step)\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    return total_loss / len(loader), global_step\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, device):\n",
                "    \"\"\"Validate on validation set.\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc=\"Validating\"):\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "        \n",
                "        outputs = model(input_ids, attention_mask, labels)\n",
                "        loss = outputs['loss']\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "print(\"✓ Training functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "best_val_loss = float('inf')\n",
                "epochs_without_improvement = 0\n",
                "global_step = 0\n",
                "\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "\n",
                "print(\"Starting training...\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for epoch in range(1, NUM_EPOCHS + 1):\n",
                "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
                "    print(\"-\" * 80)\n",
                "    \n",
                "    # Train\n",
                "    train_loss, global_step = train_epoch(\n",
                "        model, train_loader, optimizer, scheduler,\n",
                "        device, epoch, writer, global_step\n",
                "    )\n",
                "    \n",
                "    # Validate\n",
                "    val_loss = validate(model, val_loader, device)\n",
                "    \n",
                "    # Log epoch metrics\n",
                "    train_losses.append(train_loss)\n",
                "    val_losses.append(val_loss)\n",
                "    \n",
                "    writer.add_scalar('epoch/train_loss', train_loss, epoch)\n",
                "    writer.add_scalar('epoch/val_loss', val_loss, epoch)\n",
                "    \n",
                "    print(f\"Train Loss: {train_loss:.4f}\")\n",
                "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
                "    \n",
                "    # Check for improvement\n",
                "    if val_loss < best_val_loss:\n",
                "        print(f\"✓ New best! Saving model...\")\n",
                "        best_val_loss = val_loss\n",
                "        epochs_without_improvement = 0\n",
                "        \n",
                "        # Save best model\n",
                "        os.makedirs('models', exist_ok=True)\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'val_loss': val_loss,\n",
                "        }, 'models/best_model.pt')\n",
                "    else:\n",
                "        epochs_without_improvement += 1\n",
                "        print(f\"No improvement for {epochs_without_improvement} epochs\")\n",
                "    \n",
                "    # Early stopping\n",
                "    if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
                "        print(f\"\\n⚠ Early stopping triggered after {epoch} epochs\")\n",
                "        break\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"Training complete!\")\n",
                "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
                "\n",
                "writer.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualize Training Progress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot training curves\n",
                "plt.figure(figsize=(10, 6))\n",
                "epochs_range = range(1, len(train_losses) + 1)\n",
                "\n",
                "plt.plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
                "plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
                "\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training and Validation Loss')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"✓ Saved: training_curves.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Training Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save training history\n",
                "training_history = {\n",
                "    'epochs_trained': len(train_losses),\n",
                "    'train_losses': train_losses,\n",
                "    'val_losses': val_losses,\n",
                "    'best_val_loss': best_val_loss,\n",
                "    'hyperparameters': hparams,\n",
                "    'total_steps': global_step\n",
                "}\n",
                "\n",
                "with open('training_history.json', 'w') as f:\n",
                "    json.dump(training_history, f, indent=2)\n",
                "\n",
                "print(\"✓ Training history saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Training Results\n",
                "\n",
                "- **Epochs trained:** X (update after running)\n",
                "- **Best validation loss:** X.XXXX\n",
                "- **Total training steps:** X,XXX\n",
                "- **Training time:** ~X hours\n",
                "\n",
                "### Model Saved\n",
                "\n",
                "Best model checkpoint saved to: `models/best_model.pt`\n",
                "\n",
                "### Next Step: Evaluation (04-evaluation.ipynb)\n",
                "\n",
                "Now we'll:\n",
                "- Load the best model\n",
                "- Evaluate on test set\n",
                "- Compute all metrics (F1, AUC, precision, recall)\n",
                "- Create comprehensive visualizations\n",
                "- Perform ablation studies"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}